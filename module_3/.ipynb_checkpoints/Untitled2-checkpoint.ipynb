{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4103bb60b6ac>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-4103bb60b6ac>\"\u001b[1;36m, line \u001b[1;32m62\u001b[0m\n\u001b[1;33m    Подробнее по признакам:\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n",
    "!pip freeze > requirements.txt\n",
    "\n",
    "# DATA\n",
    "\n",
    "df_train = pd.read_csv('main_task.csv')\n",
    "df_test = pd.read_csv('kaggle_task.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "df_train.info()\n",
    "\n",
    "df_train.head(5)\n",
    "\n",
    "df_test.info()\n",
    "\n",
    "df_test.head(5)\n",
    "\n",
    "sample_submission.head(5)\n",
    "\n",
    "sample_submission.info()\n",
    "\n",
    "# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "\n",
    "data.info()\n",
    "\n",
    "Подробнее по признакам:\n",
    "* City: Город \n",
    "* Cuisine Style: Кухня\n",
    "* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n",
    "* Price Range: Цены в ресторане в 3 категориях\n",
    "* Number of Reviews: Количество отзывов\n",
    "* Reviews: 2 последних отзыва и даты этих отзывов\n",
    "* URL_TA: страница ресторана на 'www.tripadvisor.com' \n",
    "* ID_TA: ID ресторана в TripAdvisor\n",
    "* Rating: Рейтинг ресторана\n",
    "\n",
    "data.sample(5)\n",
    "\n",
    "Как видим, большинство признаков у нас требует очистки и предварительной обработки.\n",
    "\n",
    "data[pd.isna(data['Cuisine Style'])].info()\n",
    "\n",
    "data[pd.isna(data['Price Range'])].info()\n",
    "\n",
    "data[pd.isna(data['Number of Reviews'])].info()\n",
    "\n",
    "data[data['Reviews'] == \"[[], []]\"].info()\n",
    "\n",
    "data['Number of Reviews'].value_counts().sort_index()\n",
    "\n",
    "data[(pd.isna(data['Number of Reviews'])) & (data['Reviews'] != \"[[], []]\")]\n",
    "\n",
    "for i in data[(pd.isna(data['Number of Reviews'])) & (data['Reviews'] != \"[[], []]\")].index:\n",
    "\n",
    "    if pd.isna(data.loc[i]['Reviews']) == False:\n",
    "        l = len(data.loc[i]['Reviews'].split(','))\n",
    "        \n",
    "        if l > 2:\n",
    "            print(data.loc[i]['Reviews'])\n",
    "\n",
    "Способ может и не совсем хороший, но вполне подтверждает, что есть 1 отзыв\n",
    "\n",
    "# заполним пропуски\n",
    "data['Reviews'] = data['Reviews'].fillna(\"[[], []]\")\n",
    "\n",
    "number_of_reviews = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    if pd.isna(data.loc[i]['Number of Reviews']) == True:\n",
    "        if data.loc[i]['Reviews'] == \"[[], []]\":\n",
    "            number_of_reviews.append(0)\n",
    "        else:\n",
    "            number_of_reviews.append(1)\n",
    "    else:\n",
    "        number_of_reviews.append(data.loc[i]['Number of Reviews'])\n",
    "\n",
    "data['Number of Reviews'] = pd.Series(number_of_reviews)\n",
    "\n",
    "data.info()\n",
    "\n",
    "Для удобства заполним пропуски значениями None\n",
    "\n",
    "#data['Price Range'] = data['Price Range'].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "data['Cuisine Style'] = data['Cuisine Style'].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "data.info()\n",
    "\n",
    "добавить признак средней оценки для каждой кухни\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Обработка NAN \n",
    "У наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \n",
    "По этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак \n",
    "\n",
    "# Для примера я возьму столбец Number of Reviews\n",
    "#data['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')\n",
    "\n",
    "#data['Number_of_Reviews_isNAN'].value_counts()\n",
    "\n",
    "# Далее заполняем пропуски 0, вы можете попробовать заполнением средним или средним по городу и тд...\n",
    "#data['Number of Reviews'].fillna(0, inplace=True)\n",
    "\n",
    "#---- потом можно протестить с медианой или средним\n",
    "\n",
    "#data.info()\n",
    "\n",
    "Заполняем пропуски в столбце с кухнями\n",
    "---\n",
    "\n",
    "\n",
    "Список всех кухонь\n",
    "\n",
    "cusines = set()\n",
    "for row in data['Cuisine Style']:\n",
    "    if pd.isna(row) == False:\n",
    "        for cus in row[1:-1].replace(\"'\", '').split(','):\n",
    "            if cus[0] != ' ':\n",
    "                cusines.add(cus)\n",
    "            else:\n",
    "                cusines.add(cus[1:])\n",
    "        \n",
    "cusines.add('None')\n",
    "\n",
    "Какое среднее кол-во кухонь представлено в одном ресторане\n",
    "\n",
    "#cusine_counter_m = []\n",
    "#\n",
    "#for row in data['Cuisine Style']:\n",
    "#    if pd.isna(row) == False:\n",
    "#        cusine_counter_m.append(len(row[1:-1].replace(\"'\", '').split(',')))\n",
    "\n",
    "#pd.Series(cusine_counter_m).plot(kind='box')\n",
    "\n",
    "Целесообразне брать медиану, т.к. есть выбросы и среднее будет смещено\n",
    "\n",
    "#pd.Series(cusine_counter_m).median()\n",
    "\n",
    "Итого нам надо для каждого пропущенного значения подобрать 3 наиболее часто встречающиеся кухни.  \n",
    "Будем ориентироваться на города. Посмотрим какие кухни характерны для ресторана каждого города\n",
    "\n",
    "# составим список всех присутствующих городов\n",
    "cities = data['City'].unique()\n",
    "\n",
    "# составим словарь словарей, для дальнейшего анализа какие кухни чаще всего встречаются в городах\n",
    "# структура следующая: {Город: {Кухня: 0}}\n",
    "\n",
    "#cusines_in_city = {}\n",
    "#\n",
    "#for city in cities:\n",
    "#    interim_dict = {}\n",
    "#    \n",
    "#    for cusine in cusines:\n",
    "#        interim_dict[cusine] = 0\n",
    "#        \n",
    "#    cusines_in_city[city] = interim_dict\n",
    "\n",
    "# Созданный выше словарь словарей мы наполним значениями. \n",
    "# Посчитаем, сколько раз каждая кухня встречается в конкретном городе\n",
    "\n",
    "#for city in cities:\n",
    "#    \n",
    "#    for val in data[data['City'] == city]['Cuisine Style']:\n",
    "#        \n",
    "#        if pd.isna(val) == False:\n",
    "#        \n",
    "#            for cus in val[1:-1].replace(\"'\", '').split(','):\n",
    "#                if cus[0] != ' ':\n",
    "#                    cusines_in_city[city][cus] += 1\n",
    "#                else:\n",
    "#                    cusines_in_city[city][cus[1:]] += 1\n",
    "\n",
    "# Отберем 3 самый популярные кухни для каждого города\n",
    "\n",
    "#popular_cusines = {}\n",
    "#\n",
    "#for city in cities:\n",
    "#    popular_cus_city = pd.DataFrame(cusines_in_city)['Paris'].sort_values(ascending=False).index[:3]\n",
    "#    popular_cusines[city] = str(popular_cus_city)[6:-17]\n",
    "\n",
    "# создадим отдельный столбец с вариантами кухонь, где вместо пропусков будут часто встречаемые\n",
    "# сначала мы сделаем список, который преобразуем в пандас серию\n",
    "\n",
    "#cuisine_style_new = []\n",
    "#\n",
    "#for i in range(data.shape[0]):\n",
    "#    if pd.isna(data.loc[i]['Cuisine Style']) == True:\n",
    "#        cuisine_style_new.append(popular_cusines[data.loc[i]['City']])\n",
    "#    else:\n",
    "#        cuisine_style_new.append(data.loc[i]['Cuisine Style'])\n",
    "\n",
    "# меняем старый солбец на новый\n",
    "#data['Cuisine Style'] = pd.Series(cuisine_style_new)\n",
    "\n",
    "#data.info()\n",
    "\n",
    "### 2. Обработка признаков\n",
    "Для начала посмотрим какие признаки у нас могут быть категориальными.\n",
    "\n",
    "data.nunique(dropna=False)\n",
    "\n",
    "Интересно, что Restaurant_id имеет 13094 уникальных значений, в то время как ID_TA почти 50 тыс.  \n",
    "Скорее всего есть рестораны, которые относятся к сетям. Для таких стоит ввести отдельный признак \"Принадлежность к сети\"\n",
    "\n",
    "counter_id = data['Restaurant_id'].value_counts()\n",
    "\n",
    "## Можно заменить на кол-во ресторанов в сети\n",
    "\n",
    "counter_id['id_227']\n",
    "\n",
    "# создаем признак, где показывается кол-во ресторанов в сети\n",
    "\n",
    "data['network_restaurant'] = data['Restaurant_id'].apply(lambda x: counter_id[x])\n",
    "\n",
    "data.head(5)\n",
    "\n",
    "data.sample(5)\n",
    "\n",
    "#### Возьмем следующий признак \"Price Range\".\n",
    "\n",
    "#data['Price Range'].value_counts()\n",
    "\n",
    "По описанию 'Price Range' это - Цены в ресторане.  \n",
    "Их можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами, например 1,2,3  \n",
    "*Попробуйте сделать обработку этого признака уже самостоятельно!*\n",
    "\n",
    "# Ваша обработка 'Price Range'\n",
    "#Готовим столбец Price Range для создания dummies\n",
    "def price_range_rating (price_range_string):\n",
    "    if \"$$$$\" in str(price_range_string):\n",
    "        return(int(3))\n",
    "    elif \"$$ - $$$\" in str(price_range_string):\n",
    "        return(int(2))\n",
    "    elif \"$\" in str(price_range_string):\n",
    "        return(int(1))\n",
    "    elif str(0) in str(price_range_string):\n",
    "        return(int(0))\n",
    "data['Price Range'] = data['Price Range'].apply(price_range_rating)\n",
    "\n",
    "#Заполним пропуски наиболее встречаемым значением\n",
    "data['Price Range'] = data['Price Range'].fillna(2)\n",
    "\n",
    "#Делаем dummies на основе изменного Price Range\n",
    "#data = pd.get_dummies(data, columns=['Price Range', ], dummy_na=True)\n",
    "\n",
    "> Для некоторых алгоритмов МЛ даже для не категориальных признаков можно применить One-Hot Encoding, и это может улучшить качество модели. Пробуйте разные подходы к кодированию признака - никто не знает заранее, что может взлететь.\n",
    "\n",
    "### Обработать другие признаки вы должны самостоятельно!\n",
    "Для обработки других признаков вам возможно придется даже написать свою функцию, а может даже и не одну, но в этом и есть ваша практика в этом модуле!     \n",
    "Следуя подсказкам в модуле вы сможете более подробно узнать, как сделать эти приобразования.\n",
    "\n",
    "# Добавляем новый признак \"Столица - Не столица\"\n",
    "capitals = {'City': [\n",
    "    'London', 'Paris', 'Madrid', 'Barcelona', 'Berlin', 'Milan', 'Rome', 'Prague',\n",
    "    'Lisbon', 'Vienna', 'Amsterdam', 'Brussels', 'Hamburg', 'Munich', 'Lyon', 'Stockholm',\n",
    "    'Budapest', 'Warsaw', 'Dublin', 'Copenhagen', 'Athens', 'Edinburgh', 'Zurich', 'Oporto',\n",
    "    'Geneva', 'Krakow', 'Oslo', 'Helsinki', 'Bratislava', 'Luxembourg', 'Ljubljana'],\n",
    "    'Country': [\n",
    "    'United Kingdom', 'France', 'Spain', 'Spain', 'Germainy', 'Italy', 'Italy', 'Czech Republic',\n",
    "    'Portugal', 'Austria', 'Netherlands', 'Belgium', 'Germainy', 'Germainy', 'France', 'Sweden',\n",
    "    'Hungary', 'Poland', 'Ireland', 'Denmark', 'Greece', 'Scotland', 'Switzerland', 'Portugal',\n",
    "    'Switzerland', 'Poland', 'Norway', 'Finland', 'Slovakia', 'Luxembourg', 'Slovenia'],\n",
    "    'Is capital': [1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "}\n",
    "capitals_list = []\n",
    "for i,j in zip(capitals['City'], capitals['Is capital']):\n",
    "    if j == 1:\n",
    "        capitals_list.append(i)\n",
    "def capital_sign(city_string):\n",
    "    if city_string in capitals_list:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "data['Is_capital'] = data['City'].apply(capital_sign)\n",
    "\n",
    "#Делаем даммиз на основании нового признака\n",
    "#data = pd.get_dummies(data, columns=['Is_capital', ], dummy_na=True)\n",
    "\n",
    "#Создаем новый признак из разницы между датами отзывов\n",
    "\n",
    "# Вычленяем из строки даты и записываем в отдельный столбец\n",
    "data['Number of Reviews'].fillna(0, inplace=True)\n",
    "\n",
    "#Заполняем наны в столбце Reviews\n",
    "data['Reviews'].fillna('[[], []]', inplace=True)\n",
    "\n",
    "def data_extract(reviews_str):\n",
    "    pattern = re.compile('\\d\\d\\W\\d\\d\\W\\d\\d\\d\\d')\n",
    "    tmp_data = pattern.findall(reviews_str)\n",
    "    return(tmp_data)\n",
    "\n",
    "data['Data_Reviews'] = data['Reviews'].apply(data_extract)\n",
    "\n",
    "#Резделим даты отзывов на два столбца. Пригодится и для следующего задания\n",
    "def data_reviews_separate_1(data_reviews_string):\n",
    "    tmp = 0\n",
    "    if len(data_reviews_string) == 0:\n",
    "        tmp = 0\n",
    "    else:\n",
    "        tmp = data_reviews_string[0]\n",
    "    return(tmp)\n",
    "\n",
    "def data_reviews_separate_2(data_reviews_string):\n",
    "    tmp = 0\n",
    "    if len(data_reviews_string) == 0:\n",
    "        tmp = 0\n",
    "    elif len(data_reviews_string) == 1:\n",
    "        tmp = 0\n",
    "    elif len(data_reviews_string) == 2:\n",
    "        tmp = data_reviews_string[1]\n",
    "    return(tmp)\n",
    "\n",
    "data['Data_Reviews_1'] = data['Data_Reviews'].apply(data_reviews_separate_1)\n",
    "data['Data_Reviews_2'] = data['Data_Reviews'].apply(data_reviews_separate_2)\n",
    "\n",
    "#Преобразовываем столбцы Data_Reviews_ в формат datetime\n",
    "data['Data_Reviews_1'] = data['Data_Reviews_1'].apply(lambda x: pd.to_datetime(x) if x != 0 else False)\n",
    "data['Data_Reviews_2'] = data['Data_Reviews_2'].apply(lambda x: pd.to_datetime(x) if x != 0 else False) \n",
    "\n",
    "# Вычисляем разницу в днях между отзывами, чистим значения от None и преобразовываем в int\n",
    "def data_delta(Data_Reviews_item):\n",
    "    if (len(Data_Reviews_item) == 0) or (len(Data_Reviews_item) == 1):\n",
    "        tmp = 0\n",
    "    else:\n",
    "        tmp = pd.to_datetime(\n",
    "            Data_Reviews_item[0]) - pd.to_datetime(Data_Reviews_item[1])\n",
    "\n",
    "    if tmp != 0:\n",
    "        return(str(tmp))\n",
    "\n",
    "\n",
    "def data_delta_to_int(data_delta_item):\n",
    "    tmp = str(data_delta_item)\n",
    "    if '-' in tmp:\n",
    "        tmp1 = tmp[1:-15]\n",
    "    elif 'None' in tmp:\n",
    "        tmp1 = '0'\n",
    "    else:\n",
    "        tmp1 = tmp[:-14]\n",
    "    return(int(tmp1))\n",
    "\n",
    "\n",
    "data['Data_Delta'] = data['Data_Reviews'].apply(data_delta)\n",
    "data['Data_Delta'] = data['Data_Delta'].apply(data_delta_to_int)\n",
    "\n",
    "\n",
    "#Убираем пропуски из столбцов Data_Reviews_х\n",
    "def error_date_to_zero(data_string):\n",
    "    if data_string == False:\n",
    "        return(int(0))\n",
    "    else:\n",
    "        return(data_string)\n",
    "\n",
    "data['Data_Reviews_1'] = data['Data_Reviews_1'].apply(error_date_to_zero)\n",
    "data['Data_Reviews_2'] = data['Data_Reviews_2'].apply(error_date_to_zero)\n",
    "\n",
    "Добавим признак - кол-во кухонь предоставленных в ресторане\n",
    "\n",
    "def cusine_counter(row):\n",
    "    cusine = []\n",
    "    for cus in row[1:-1].replace(\"'\", '').split(','):\n",
    "        cusine.append(cus)\n",
    "    return len(cusine)\n",
    "\n",
    "data['cusine_count'] = data['Cuisine Style'].apply(cusine_counter)\n",
    "\n",
    "Добавим признак - кол-во ресторанов в городе\n",
    "\n",
    "quantity_rest = data['City'].value_counts()\n",
    "\n",
    "data['quantity'] = data['City'].apply(lambda x: quantity_rest[x])\n",
    "\n",
    "Следующий признак - средняя оценка в городе  \n",
    "*Можно потом поменять на отклонение от средней оценки, если не устроят результаты*\n",
    "\n",
    "mean_rating_in_city = data.groupby('City')['Rating'].mean()\n",
    "\n",
    "data['mean_rating'] = data['City'].apply(lambda x: mean_rating_in_city[x])\n",
    "\n",
    "Признак - среднее кол-во отзывов о ресторане в городе\n",
    "\n",
    "mean_number_reviews_in_city = data.groupby('City')['Number of Reviews'].mean()\n",
    "\n",
    "data['mean_reviews'] = data['City'].apply(lambda x: mean_number_reviews_in_city[x])\n",
    "\n",
    "Создадим даммиз по кухням\n",
    "\n",
    "#cuisines_rating = {}\n",
    "#\n",
    "#for cus in cusines:\n",
    "#    cu_rating = data[data['Cuisine Style'].str.contains(cus)]['Rating'].median()\n",
    "#    cuisines_rating[cus] = cu_rating\n",
    "\n",
    "#def find_cuisine(row):\n",
    "#    if cus in row:\n",
    "#        return cuisines_rating[cus]\n",
    "#    return 0\n",
    "\n",
    "#for cus in list(cusines):\n",
    "#    data[cus] = data['Cuisine Style'].apply(find_cuisine)\n",
    "\n",
    "# EDA \n",
    "[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Анализ данных\n",
    "На этом этапе мы строим графики, ищем закономерности, аномалии, выбросы или связи между признаками.\n",
    "В общем цель этого этапа понять, что эти данные могут нам дать и как признаки могут быть взаимосвязаны между собой.\n",
    "Понимание изначальных признаков позволит сгенерировать новые, более сильные и, тем самым, сделать нашу модель лучше.\n",
    "![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png)\n",
    "\n",
    "### Посмотрим распределение признака\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "df_train['Ranking'].hist(bins=100)\n",
    "\n",
    "У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?\n",
    "\n",
    "df_train['City'].value_counts(ascending=True).plot(kind='barh')\n",
    "\n",
    "А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:\n",
    "\n",
    "df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)\n",
    "\n",
    "# посмотрим на топ 10 городов\n",
    "for x in (df_train['City'].value_counts())[0:10].index:\n",
    "    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\n",
    "plt.show()\n",
    "\n",
    "Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n",
    "\n",
    ">Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n",
    "\n",
    "\n",
    "data['Ranking_global'] = data['Ranking'] / data['City'].apply(lambda x: quantity_rest[x])\n",
    "\n",
    "data['Ranking_global'].hist(bins=100)\n",
    "\n",
    "### Посмотрим распределение целевой переменной\n",
    "\n",
    "df_train['Rating'].value_counts(ascending=True).plot(kind='barh')\n",
    "\n",
    "### Посмотрим распределение целевой переменной относительно признака\n",
    "\n",
    "df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)\n",
    "\n",
    "df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)\n",
    "\n",
    "### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\n",
    "На этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной.\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (15,10)\n",
    "#sns.heatmap(data.drop(['sample'], axis=1).corr(),)\n",
    "\n",
    "Вообще благодаря визуализации в этом датасете можно узнать много интересных фактов, например:\n",
    "* где больше Пицерий в Мадриде или Лондоне?\n",
    "* в каком городе кухня ресторанов более разнообразна?\n",
    "\n",
    "придумайте свои вопрос и найдите на него ответ в данных)\n",
    "\n",
    "#### Запускаем и проверяем что получилось\n",
    "\n",
    "#df_preproc = preproc_data(data)\n",
    "#df_preproc.sample(10)\n",
    "\n",
    "#df_preproc.info()\n",
    "\n",
    "# Теперь выделим тестовую часть\n",
    "train_data = data.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = data.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)\n",
    "\n",
    "X = X.drop(['Restaurant_id', 'City', 'Cuisine Style', 'Reviews', 'URL_TA', 'ID_TA'], axis=1)\n",
    "\n",
    "X = X.drop(['Data_Reviews', 'Data_Reviews_1', 'Data_Reviews_2'], axis=1)\n",
    "\n",
    "**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \n",
    "Это поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**\n",
    "\n",
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape\n",
    "\n",
    "# Model \n",
    "Сам ML\n",
    "\n",
    "# Импортируем необходимые библиотеки:\n",
    "from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\n",
    "from sklearn import metrics # инструменты для оценки точности модели\n",
    "\n",
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f=f\n",
    "\n",
    "# Submission\n",
    "Если все устраевает - готовим Submission на кагл\n",
    "\n",
    "test_data.sample(10)\n",
    "\n",
    "test_data = test_data.drop(['Rating'], axis=1)\n",
    "\n",
    "sample_submission\n",
    "\n",
    "predict_submission = model.predict(test_data)\n",
    "\n",
    "predict_submission\n",
    "\n",
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head(10)\n",
    "\n",
    "# What's next?\n",
    "Или что делать, чтоб улучшить результат:\n",
    "* Обработать оставшиеся признаки в понятный для машины формат\n",
    "* Посмотреть, что еще можно извлечь из признаков\n",
    "* Сгенерировать новые признаки\n",
    "* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n",
    "* Подобрать состав признаков\n",
    "\n",
    "В общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "Теперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию.\n",
    "\n",
    "# на всякий случай, заново подгружаем данные\n",
    "df_train = pd.read_csv(DATA_DIR+'/main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "data.info()\n",
    "\n",
    "def preproc_data(df_input):\n",
    "    '''includes several functions to pre-process the predictor data.'''\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    # ################### 1. Предобработка ############################################################## \n",
    "    # убираем не нужные для модели признаки\n",
    "    df_output.drop(['Restaurant_id','ID_TA',], axis = 1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # ################### 2. NAN ############################################################## \n",
    "    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n",
    "    df_output['Number of Reviews'].fillna(0, inplace=True)\n",
    "    # тут ваш код по обработке NAN\n",
    "    # ....\n",
    "    \n",
    "    \n",
    "    # ################### 3. Encoding ############################################################## \n",
    "    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n",
    "    #df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n",
    "    \n",
    "    # ################### 4. Feature Engineering ####################################################\n",
    "    # тут ваш код не генерацию новых фитчей\n",
    "    #Готовим столбец Price Range для создания dummies\n",
    "    def price_range_rating (price_range_string):\n",
    "        if \"$$$$\" in str(price_range_string):\n",
    "            return(int(3))\n",
    "        elif \"$$ - $$$\" in str(price_range_string):\n",
    "            return(int(2))\n",
    "        elif \"$\" in str(price_range_string):\n",
    "            return(int(1))\n",
    "        elif str(0) in str(price_range_string):\n",
    "            return(int(0))\n",
    "    df_output['Price Range'] = df_output['Price Range'].apply(price_range_rating)\n",
    "    #Делаем dummies на основе изменного Price Range\n",
    "    df_output = pd.get_dummies(df_output, columns=['Price Range', ], dummy_na=True)\n",
    "    \n",
    "    # Добавляем новый признак \"Столица - Не столица\"\n",
    "    capitals = {'City': [\n",
    "    'London', 'Paris', 'Madrid', 'Barcelona', 'Berlin', 'Milan', 'Rome', 'Prague',\n",
    "    'Lisbon', 'Vienna', 'Amsterdam', 'Brussels', 'Hamburg', 'Munich', 'Lyon', 'Stockholm',\n",
    "    'Budapest', 'Warsaw', 'Dublin', 'Copenhagen', 'Athens', 'Edinburgh', 'Zurich', 'Oporto',\n",
    "    'Geneva', 'Krakow', 'Oslo', 'Helsinki', 'Bratislava', 'Luxembourg', 'Ljubljana'],\n",
    "    'Country': [\n",
    "    'United Kingdom', 'France', 'Spain', 'Spain', 'Germainy', 'Italy', 'Italy', 'Czech Republic',\n",
    "    'Portugal', 'Austria', 'Netherlands', 'Belgium', 'Germainy', 'Germainy', 'France', 'Sweden',\n",
    "    'Hungary', 'Poland', 'Ireland', 'Denmark', 'Greece', 'Scotland', 'Switzerland', 'Portugal',\n",
    "    'Switzerland', 'Poland', 'Norway', 'Finland', 'Slovakia', 'Luxembourg', 'Slovenia'],\n",
    "    'Is capital': [1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "    }\n",
    "    capitals_list = []\n",
    "    for i,j in zip(capitals['City'], capitals['Is capital']):\n",
    "        if j == 1:\n",
    "            capitals_list.append(i)\n",
    "    def capital_sign(city_string):\n",
    "        if city_string in capitals_list:\n",
    "            return(1)\n",
    "        else:\n",
    "            return(0)\n",
    "    df_output['Is_capital'] = df_output['City'].apply(capital_sign)\n",
    "    #Делаем даммиз на основании нового признака\n",
    "    df_output = pd.get_dummies(df_output, columns=['Is_capital', ], dummy_na=True)\n",
    "    \n",
    "\n",
    "    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n",
    "    df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n",
    "    \n",
    "    #Создаем новый признак из разницы между датами отзывов\n",
    "\n",
    "    # Вычленяем из строки даты и записываем в отдельный столбец\n",
    "    df_output['Number of Reviews'].fillna(0, inplace=True)\n",
    "    #Заполняем наны в столбце Reviews\n",
    "    df_output['Reviews'].fillna('[[], []]', inplace=True)\n",
    "    def data_extract(reviews_str):\n",
    "        pattern = re.compile('\\d\\d\\W\\d\\d\\W\\d\\d\\d\\d')\n",
    "        tmp_data = pattern.findall(reviews_str)\n",
    "        return(tmp_data)\n",
    "    df_output['Data_Reviews'] = df_output['Reviews'].apply(data_extract)\n",
    "    #Резделим даты отзывов на два столбца. Пригодится и для следующего задания\n",
    "    def data_reviews_separate_1(data_reviews_string):\n",
    "        tmp = 0\n",
    "        if len(data_reviews_string) == 0:\n",
    "            tmp = 0\n",
    "        else:\n",
    "            tmp = data_reviews_string[0]\n",
    "        return(tmp)\n",
    "\n",
    "    def data_reviews_separate_2(data_reviews_string):\n",
    "        tmp = 0\n",
    "        if len(data_reviews_string) == 0:\n",
    "            tmp = 0\n",
    "        elif len(data_reviews_string) == 1:\n",
    "            tmp = 0\n",
    "        elif len(data_reviews_string) == 2:\n",
    "            tmp = data_reviews_string[1]\n",
    "        return(tmp)\n",
    "\n",
    "    df_output['Data_Reviews_1'] = df_output['Data_Reviews'].apply(data_reviews_separate_1)\n",
    "    df_output['Data_Reviews_2'] = df_output['Data_Reviews'].apply(data_reviews_separate_2)\n",
    "\n",
    "    #Преобразовываем столбцы Data_Reviews_ в формат datetime\n",
    "    df_output['Data_Reviews_1'] = df_output['Data_Reviews_1'].apply(lambda x: pd.to_datetime(x) if x != 0 else False)\n",
    "    df_output['Data_Reviews_2'] = df_output['Data_Reviews_2'].apply(lambda x: pd.to_datetime(x) if x != 0 else False) \n",
    "\n",
    "    # Вычисляем разницу в днях между отзывами, чистим значения от None и преобразовываем в int\n",
    "    def data_delta(Data_Reviews_item):\n",
    "        if (len(Data_Reviews_item) == 0) or (len(Data_Reviews_item) == 1):\n",
    "            tmp = 0\n",
    "        else:\n",
    "            tmp = pd.to_datetime(\n",
    "                Data_Reviews_item[0]) - pd.to_datetime(Data_Reviews_item[1])\n",
    "\n",
    "        if tmp != 0:\n",
    "            return(str(tmp))\n",
    "\n",
    "\n",
    "    def data_delta_to_int(data_delta_item):\n",
    "        tmp = str(data_delta_item)\n",
    "        if '-' in tmp:\n",
    "            tmp1 = tmp[1:-15]\n",
    "        elif 'None' in tmp:\n",
    "            tmp1 = '0'\n",
    "        else:\n",
    "            tmp1 = tmp[:-14]\n",
    "        return(int(tmp1))\n",
    "\n",
    "\n",
    "    df_output['Data_Delta'] = df_output['Data_Reviews'].apply(data_delta)\n",
    "    df_output['Data_Delta'] = df_output['Data_Delta'].apply(data_delta_to_int)\n",
    "\n",
    "\n",
    "    #Убираем пропуски из столбцов Data_Reviews_х\n",
    "    def error_date_to_zero(data_string):\n",
    "        if data_string == False:\n",
    "            return(int(0))\n",
    "        else:\n",
    "            return(data_string)\n",
    "    df_output['Data_Reviews_1'] = df_output['Data_Reviews_1'].apply(error_date_to_zero)\n",
    "    df_output['Data_Reviews_2'] = df_output['Data_Reviews_2'].apply(error_date_to_zero)\n",
    "    \n",
    "    #Создаем даммиз из столбца с набором кухонь в ресторане\n",
    "\n",
    "    #Собираем множество из значений столбца data['Cuisine']\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    cuisine_set = set()\n",
    "    def clearing_string(cuisine_str):\n",
    "        tmp = str(cuisine_str).replace('\\'',\"\")\n",
    "        tmp = tmp.replace(' ',\"\")\n",
    "        tmp = tmp[1:-1]\n",
    "        tmp_lst = tmp.split(',')\n",
    "        for i in range(0,len(tmp_lst)):\n",
    "            cuisine_set.add(tmp_lst[i])\n",
    "        return(cuisine_set)\n",
    "    df_output['Cuisine Style'].apply(clearing_string)\n",
    "    cuisine_set.remove('a')\n",
    "\n",
    "    #Создаем даммиз из столбца с набором кухонь в ресторане\n",
    "\n",
    "    #Собираем множество из значений столбца data['Cuisine']\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    cuisine_set = set()\n",
    "    def clearing_string(cuisine_str):\n",
    "        tmp = str(cuisine_str).replace('\\'',\"\")\n",
    "        tmp = tmp.replace(' ',\"\")\n",
    "        tmp = tmp[1:-1]\n",
    "        tmp_lst = tmp.split(',')\n",
    "        for i in range(0,len(tmp_lst)):\n",
    "            cuisine_set.add(tmp_lst[i])\n",
    "        return(cuisine_set)\n",
    "    df_output['Cuisine Style'].apply(clearing_string)\n",
    "    cuisine_set.remove('a')\n",
    "\n",
    "\n",
    "    #Для начала преобразуем элементы столбца Cuisine из строк в списки\n",
    "    def cuisine_convert_to_list (cuisine_str):\n",
    "        tmp = str(cuisine_str).replace('\\'',\"\")\n",
    "        tmp = tmp.replace(' ',\"\")\n",
    "        tmp = tmp[1:-1]\n",
    "        tmp_lst = tmp.split(',')\n",
    "        return(tmp_lst)\n",
    "    #Преобразовываем элементы слобца 'Cuisine' из строк в списки\n",
    "    df_output['Cuisine Style'] = df_output['Cuisine Style'].apply(cuisine_convert_to_list)\n",
    "\n",
    "    #Создадим словарь из ключей - названий кухнь с нулевыми начениями\n",
    "    cuisine_list = list(cuisine_set)\n",
    "    cuisine_df = pd.DataFrame(cuisine_list)\n",
    "    cuisine_df = cuisine_df[0].sort_values()\n",
    "    cuisine_df = cuisine_df.reset_index(drop = True)\n",
    "    #Добавляем новые столбцы по именам кухонь для будущих признаков\n",
    "    for i in range (0, len(cuisine_list)):\n",
    "        df_output[cuisine_df[i]] = int(0)\n",
    "\n",
    "    #Проверяем есть ли имя столбца, одноименного с именем кухни, в строке \"Cuisine\". Если да, в соотв. ячеку столбца с именем кухни ставим 1\n",
    "    for i in range (0, len(cuisine_df)):\n",
    "        for j in range (0,len(df_output)):\n",
    "            if cuisine_df[i] in df_output['Cuisine Style'][j]:\n",
    "                df_output[cuisine_df[i]][j] = int(1)\n",
    "\n",
    "# ################### 5. Clean #################################################### \n",
    "# убираем признаки которые еще не успели обработать, \n",
    "# модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n",
    "    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n",
    "    df_output.drop(object_columns, axis = 1, inplace=True)\n",
    "    return df_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
